# ğŸš€ ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression
[![arXiv](https://img.shields.io/badge/arXiv-2310.12345-b31b1b.svg)](https://arxiv.org/abs/2602.11008)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![HuggingFace Collection](https://img.shields.io/badge/HuggingFace-Collection-yellow.svg?logo=huggingface)]()
[![Kaggle](https://img.shields.io/badge/Kaggle-Notebook-20beff?logo=kaggle&logoColor=white)]()
![ROCKET Architecture](figs/logo.png)
In a quiet corner of the AI research lab, a cartoon rocket stood on the launchpadâ€”bright red, cheerful, and boldly labeled â€œLLM.â€ At the control console sat a scientist, fingers hovering over a single, enormous red button marked â€œSolve MCKP.â€
With a deep breath and a flicker of hope, they pressed it.
The rocket roared to life. Flames erupted, scattering clouds of sparse matrices like confetti made of zeros. As the LLM blasted into the stratosphere of efficient inference, it left behind on the pad a humble knapsack overflowing not with gold, but with perfectly balanced (rank, sparsity) pairs: the optimal solutions to the Multiple-Choice Knapsack Problem, handpicked for model compression.
Up it soared lighter, faster, smarter carrying only what truly mattered.
```
rocket/
â”œâ”€â”€ setup.py
â”œâ”€â”€ rocket/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ default.yaml
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prepare_data.py          # prepare_data logic (calibration data activations)
â”‚   â”œâ”€â”€ calib/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ calib.py                 # Calib class (Calib.build_calibration_dataset, Calib.get_s_inv_s, etc.) (Whitening transform)
â”‚   â”œâ”€â”€ profiling/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ profiler.py              # profile_all_layers, get_k_and_sparsity, etc. (for dynamic budget allocation)
â”‚   â”œâ”€â”€ compression/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rocket.py              # svd_with_magnitude_sparsity_on_v, model patching
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ seed.py                  # seed_all
â”‚   â”‚   â”œâ”€â”€ model_utils.py           # get_weight_transposed, compute_actual_compression
â”‚   â”‚   â””â”€â”€ io.py                    # JSON save/load helpers
â”‚   â”œâ”€â”€ scripts/
â”‚       â”œâ”€â”€ gather_activations.py
â”‚       â”œâ”€â”€ profile_layers.py
â”‚       â”œâ”€â”€ compress_model.py
â”‚       â”œâ”€â”€ evaluate_model.py
â”‚       â””â”€â”€ run_full_pipeline.py
â””â”€â”€ README.md
```
## Installation
We highly recommend using this docker image to ensure reproducability.
```
pytorch/pytorch:2.7.1-cuda12.6-cudnn9-devel 
```
Then run 
```bash
pip install -e .
```
## Running
We provide multiple console entrypoints to run the full pipeline you can easily do 
```bash
rocket-run-pipeline --config "./rocket/config/default.yaml"
```
you can use the sample <a href="./rocket/config/default.yaml">config</a> fie and modify it according to your requirements 
Other entrypoint are:
```bash
rocket-profile-layers --config CONFIG # To do profiling only
rocket-compress --config CONFIG #run compression only
rocket-evaluate --config CONFIG # Evaluation only
rocket-gather-activations --config CONFIG # Prepare Calibration data
```

## Inference optimized
Note that we provide in extra folder a modeling file to run the optimized verison which includes implementation of Macko and fuzed layers. 
to use the optimized version after you finish compression you load the model from the modeling file and call optimize
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from modeling_llama_svdllm_opt import LlamaForCausalLM
model = LlamaForCausalLM.from_pretrained("MODEL_PATH", device_map="cuda", torch_dtype="float16", compression_path="./cr_llama.json")
tokenizer = AutoTokenizer.from_pretrained("MODEL_PATH")
model.optimize()
model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
```
If you run without calling optimize you will be running the trivial implementation
